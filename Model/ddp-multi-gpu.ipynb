{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Distributed Data Parallel - Multi GPU\n","\n","We are performing DDP by distributing the data across multiple GPU. We are using the pretrained ResNet 50 Model. The model is trained on the spectrogram images generated from EEG signal data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:30:55.728379Z","iopub.status.busy":"2024-04-19T02:30:55.727506Z","iopub.status.idle":"2024-04-19T02:31:04.186679Z","shell.execute_reply":"2024-04-19T02:31:04.185354Z","shell.execute_reply.started":"2024-04-19T02:30:55.728350Z"},"trusted":true},"outputs":[],"source":["import os\n","import shutil\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision import datasets, models\n","from torch import nn, optim\n","from torch.utils.data import DataLoader, Dataset\n","import torch.nn as nn\n","from torch.cuda.amp import GradScaler, autocast\n","from torchvision.models import resnet50\n","import torch.multiprocessing as mp\n","import torch.distributed as dist\n","import torch.optim as optim\n","import torch.nn.functional as F \n","import matplotlib.pyplot as plt\n","import dask.array as da\n","import dask.dataframe as dd\n","from dask.distributed import Client, LocalCluster\n","import time"]},{"cell_type":"markdown","metadata":{},"source":["### Data Setup for Model Training"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:31:04.188870Z","iopub.status.busy":"2024-04-19T02:31:04.187914Z","iopub.status.idle":"2024-04-19T02:45:04.751117Z","shell.execute_reply":"2024-04-19T02:45:04.750154Z","shell.execute_reply.started":"2024-04-19T02:31:04.188841Z"},"trusted":true},"outputs":[],"source":["# Split the dataset into train, validation and test\n","def create_splits(data_dir, output_dir, train_size=0.7, val_size=0.15, test_size=0.15):\n","    classes = [d for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n","    splits = {'train': train_size, 'val': val_size, 'test': test_size}\n"," \n","    assert sum(splits.values()) == 1, \"Sum of split sizes should be 1.\"\n"," \n","    # Output directories for the splits\n","    for split in splits.keys():\n","        for cls in classes:\n","            os.makedirs(os.path.join(output_dir, split, cls), exist_ok=True)\n"," \n","    # Iterate over each class directory\n","    for cls in classes:\n","        class_dir = os.path.join(data_dir, cls)\n","        images = [img for img in os.listdir(class_dir) if img.lower().endswith(('png', 'jpg', 'jpeg'))]\n","        # Stratified splits\n","        train_val, test = train_test_split(images, test_size=splits['test'], random_state=42, stratify=None)\n","        train, val = train_test_split(train_val, test_size=splits['val'] / (splits['train'] + splits['val']), random_state=42, stratify=None)\n"," \n","        # Function to copy files to directories\n","        def copy_files(files, split):\n","            for f in files:\n","                src = os.path.join(class_dir, f)\n","                dst = os.path.join(output_dir, split, cls, f)\n","                shutil.copy(src, dst)\n"," \n","        # Copy files to respective split directories\n","        copy_files(train, 'train')\n","        copy_files(val, 'val')\n","        copy_files(test, 'test')\n"," \n"," \n","data_dir = '/kaggle/input/dataset24/dataset_24'\n","output_dir = '/kaggle/working/dataset_splits/'\n","create_splits(data_dir, output_dir)"]},{"cell_type":"markdown","metadata":{},"source":["### Data Augmentation using Dask"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:45:04.752526Z","iopub.status.busy":"2024-04-19T02:45:04.752271Z","iopub.status.idle":"2024-04-19T02:45:04.760525Z","shell.execute_reply":"2024-04-19T02:45:04.759547Z","shell.execute_reply.started":"2024-04-19T02:45:04.752503Z"},"trusted":true},"outputs":[],"source":["# Data Augmentation using dask\n","class CustomDataset(Dataset):\n","    def __init__(self, ddf, transform=None):\n","        self.ddf = ddf\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.ddf)\n","\n","    def __getitem__(self, idx):\n","        row = self.ddf.iloc[idx]\n","        image_path = row['path']\n","        class_label = row['class']\n","        transformed_image = row['transformed_image']\n","        if isinstance(transformed_image, str):\n","            image = Image.open(transformed_image)\n","        else:\n","            image = Image.fromarray(transformed_image)\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, class_label\n","\n","def add_augmented_images(root_dir, ddf):\n","\n","    # check for data folder\n","    os.makedirs(root_dir, exist_ok=True)\n","    # iterate over dask dataframe containing transformed images\n","    for _, row in ddf.iterrows():\n","        class_name = row['class']\n","        image_path = row['path']\n","        class_dir = os.path.join(root_dir, class_name)\n","        os.makedirs(class_dir, exist_ok=True)\n","        image_filename = os.path.basename(image_path)\n","        image_dest_path = os.path.join(class_dir, image_filename)\n","        if not os.path.exists(image_dest_path):\n","            # move images to the respective directory\n","            shutil.copy(image_path, image_dest_path)\n","\n","def get_data_loader(batch_size, data_path, train=True):\n","    print(\"get data loader\")\n","    if train:\n","        transform = transforms.Compose([\n","       transforms.RandomResizedCrop(224),\n","         transforms.RandomHorizontalFlip(),\n","         transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","         transforms.RandomRotation(10),\n","         transforms.ToTensor(),\n","         transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","     ])\n","\n","        image_paths = []\n","        for class_name in os.listdir(data_path):\n","            print(class_name)\n","            class_dir = os.path.join(data_path, class_name)\n","            for sample_name in os.listdir(class_dir):\n","                image_path = os.path.join(class_dir, sample_name)\n","                image_paths.append((class_name, image_path))\n","        # Using Dask Dataframes\n","        ddf = dd.from_pandas(pd.DataFrame(image_paths, columns=['class', 'path']), npartitions=4)\n","        # Create a custom dataset instance using your Dask DataFrame\n","        dataset = CustomDataset(ddf, transform=transform)\n","        add_augmented_images(data_path, ddf)\n","        # load data after augmentation\n","        dataset = datasets.ImageFolder(data_path, transform=transform)\n","\n","    else:\n","        transform = transforms.Compose([\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n","        ])\n","        dataset = datasets.ImageFolder(data_path, transform=transform)\n","    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=train, num_workers=4, pin_memory=True)\n","    return data_loader"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:45:04.762933Z","iopub.status.busy":"2024-04-19T02:45:04.762672Z","iopub.status.idle":"2024-04-19T02:45:04.779116Z","shell.execute_reply":"2024-04-19T02:45:04.778365Z","shell.execute_reply.started":"2024-04-19T02:45:04.762911Z"},"trusted":true},"outputs":[],"source":["# Data directories\n","data_dir = '/kaggle/working/dataset_splits'\n","train_dir = f'{data_dir}/train'\n","val_dir = f'{data_dir}/val'\n","test_dir = f'{data_dir}/test'"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:59:14.941707Z","iopub.status.busy":"2024-04-19T02:59:14.940952Z","iopub.status.idle":"2024-04-19T02:59:14.964741Z","shell.execute_reply":"2024-04-19T02:59:14.963667Z","shell.execute_reply.started":"2024-04-19T02:59:14.941671Z"},"trusted":true},"outputs":[],"source":["# Setup DDP\n","def setup(rank, world_size):\n","    os.environ['MASTER_ADDR'] = 'localhost'\n","    os.environ['MASTER_PORT'] = '12355'\n","    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n","    torch.cuda.set_device(rank)\n","# Function to save model checkpoints\n","def save_checkpoint(model, optimizer, epoch, filename=\"checkpoint.pth\"):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model_state_dict': model.module.state_dict(),  # Note: unwrap the model from DDP\n","        'optimizer_state_dict': optimizer.state_dict(),\n","    }\n","    torch.save(checkpoint, filename)\n"," # Train model using AMP \n","def train(model, loader, optimizer, criterion, scaler, rank, epoch, save_interval):\n","    model.train()\n","    total_loss = 0.0\n","    for batch_idx, (data, target) in enumerate(loader):\n","        data, target = data.to(rank), target.to(rank)\n","        optimizer.zero_grad()\n","        with autocast():  # automatic mixed precision\n","            output = model(data)\n","            loss = criterion(output, target)\n","           \n","           # losses = [torch.tensor(loss).to(rank) for _ in range(2)]\n","           # print(f\"Loss : {losses}\")\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        total_loss += loss.item()\n","        # if rank == 0 and batch_idx % save_interval == 0:\n","        #     save_checkpoint(model, optimizer, epoch, filename=f\"checkpoint_epoch_{epoch}_batch_{batch_idx}.pth\")\n","    return total_loss / len(loader), loss\n"," \n","# Validation Function\n","def validate_model(model, val_loader, rank):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, labels in val_loader:\n","            images, labels = images.to(rank), labels.to(rank)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs.data, 1)\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n"," \n","    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n"," \n"," \n","# Plot training loss curves\n","def Training_Loss(epochs, train_loss_gpu1, train_loss_gpu2, train_loss_overall):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(epochs, train_loss_gpu1, label='GPU 1')\n","    plt.plot(epochs, train_loss_gpu2, label='GPU 2')\n","    plt.plot(epochs, train_loss_overall, label='Overall')\n","    plt.title('Training Loss Curves')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    plt.grid(True)\n","    plt.savefig('DDP_Train_Loss_GPUs.png') \n","    plt.show()\n","    \n"," # Perform training using DDP on multi-GPUs using ResNet50\n","def ddp_train(rank, world_size, epochs):\n","    global train_dir, val_dir\n","    train_loss_gpu1 = []\n","    train_loss_gpu2 = []\n","    train_loss_overall = []\n","    setup(rank, world_size)\n","    model = resnet50().to(rank)\n","    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n","    batch_size = 256\n","    train_loader = get_data_loader(batch_size // world_size, train_dir, train=True)\n","    val_loader = get_data_loader(batch_size // world_size, val_dir, train=False)\n","    criterion = nn.CrossEntropyLoss().to(rank)\n","    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n","    scaler = GradScaler()  # Initialize the gradient scaler for AMP\n","    save_interval = 100  # Interval for saving checkpoints\n"," \n","    for epoch in range(epochs):\n","        print(\"Epoch : \", epoch)\n","        ovr_loss, indi_loss = train(model, train_loader, optimizer, criterion, scaler, rank, epoch, save_interval)\n","        if rank == 0:\n","            print(f\"Epoch {epoch+1}, Loss: {ovr_loss}\")\n","            save_checkpoint(model, optimizer, epoch, filename=f\"checkpoint_epoch_{epoch}.pth\")\n","\n","        # Gather loss from all GPUs\n","        losses = [torch.tensor(indi_loss).to(rank) for _ in range(world_size)]\n","        torch.distributed.all_gather(losses, indi_loss)\n","\n","        # Store loss values from each GPU\n","        train_loss_gpu1.append(losses[0].item())  # Loss on GPU 1\n","        train_loss_gpu2.append(losses[1].item())  # Loss on GPU 2\n","\n","        # Compute overall loss\n","        train_loss_overall.append(ovr_loss)\n","        if rank == 0:\n","            print(f\"Epoch {epoch+1}, Loss: {ovr_loss}\")\n","            save_checkpoint(model, optimizer, epoch, filename=f\"checkpoint_epoch_{epoch}.pth\")\n","            validate_model(model, val_loader, rank)\n","            \n","    # plot training loss on both the GPUs\n","    epochs = range(1, 11)\n","    Training_Loss(epochs, train_loss_gpu1, train_loss_gpu2, train_loss_overall)\n"," \n","    dist.destroy_process_group()\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-04-19T02:59:18.334186Z","iopub.status.busy":"2024-04-19T02:59:18.333282Z","iopub.status.idle":"2024-04-19T06:04:33.349191Z","shell.execute_reply":"2024-04-19T06:04:33.347994Z","shell.execute_reply.started":"2024-04-19T02:59:18.334151Z"},"trusted":true},"outputs":[],"source":["# Training ResNet 50 pre-trained model on the custom spectrogram data using DDP on 2 GPUs\n"," \n","if __name__ == \"__main__\":\n","    world_size = 2 \n","    epochs = 10\n","\n","    processes = []\n","\n","    for rank in range(world_size):\n","        p = mp.Process(target=ddp_train, args=(rank, world_size, epochs))\n","        p.start()\n","        processes.append(p)\n"," \n","    for p in processes:\n","        p.join()"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4820270,"sourceId":8150351,"sourceType":"datasetVersion"}],"dockerImageVersionId":30699,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
